{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataKrj/AI-project-2024/blob/main/Step_1_Step_2_Step_3_sanction_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Step -Sanction list check"
      ],
      "metadata": {
        "id": "ZKnITnhz0rKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhxrU0ZFUugS",
        "outputId": "0ed0a12d-1c1a-4672-eea1-5358f1bfac14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanctioned names saved to 'sanction_list.csv'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# URLs for the CSV files\n",
        "url1 = 'https://www.treasury.gov/ofac/downloads/sdn.csv'\n",
        "url2 = 'https://www.treasury.gov/ofac/downloads/consolidated/cons_alt.csv'\n",
        "\n",
        "# Read data from the first CSV file\n",
        "df1 = pd.read_csv(url1, on_bad_lines='skip')\n",
        "sanction_list_url1 = df1.iloc[:, 1].dropna().unique()\n",
        "\n",
        "# Read data from the second CSV file\n",
        "df2 = pd.read_csv(url2, on_bad_lines='skip')\n",
        "sanction_list_url2 = df2.iloc[:, 3].dropna().unique()\n",
        "\n",
        "# Combine the names from both CSV files\n",
        "sanction_list = list(set(sanction_list_url1) | set(sanction_list_url2))\n",
        "\n",
        "# Create a DataFrame from the combined names\n",
        "sanction_list_df = pd.DataFrame({'Sanctioned Names': sanction_list})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "sanction_list_df.to_csv('sanction_list.csv', index=False)\n",
        "\n",
        "print(\"Sanctioned names saved to 'sanction_list.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Paths to the files\n",
        "uploaded_file_path = 'BELGIUM_companies_short.xlsx'\n",
        "sanction_list_file_path = 'sanction_list.csv'\n",
        "\n",
        "# Load the companies file\n",
        "companies_df = pd.read_excel(uploaded_file_path)\n",
        "\n",
        "# Load the sanction list\n",
        "sanction_list_df = pd.read_csv(sanction_list_file_path)\n",
        "\n",
        "# Normalize the sanction list for case-insensitive matching\n",
        "sanctioned_names = sanction_list_df['Sanctioned Names'].str.lower().tolist()\n",
        "\n",
        "# Function for approximate matching\n",
        "def approximate_match(name, sanctioned_names, threshold=85):\n",
        "    \"\"\" Check if a name approximately matches any sanctioned name.\n",
        "    : name: Name to match\n",
        "    :sanctioned_names: List of sanctioned names\n",
        "    :threshold: Minimum similarity score for a match\n",
        "    : 30 if a match is found, 0 otherwise\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    for sanctioned_name in sanctioned_names:\n",
        "        similarity = fuzz.ratio(name, sanctioned_name)\n",
        "        if similarity >= threshold:\n",
        "            return 30  # Match found\n",
        "    return 0  # No match\n",
        "\n",
        "# Evaluate if company names approximately match any name in the sanction list\n",
        "companies_df['Score_Step_1'] = companies_df['Name'].apply(\n",
        "    lambda name: approximate_match(name, sanctioned_names)\n",
        ")\n",
        "\n",
        "# Save the updated  new file\n",
        "output_file_path = 'Step_1_evaluated_companies.xlsx'\n",
        "companies_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Evaluation complete with approximate matching. Results saved to {output_file_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udXl_T6-2r1x",
        "outputId": "0d18d9d0-e02d-47d8-d4ba-780e3bb6084d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Evaluation complete with approximate matching. Results saved to Step_1_evaluated_companies.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II STEP- Company status/ active check"
      ],
      "metadata": {
        "id": "5eVlnOsGAtwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from time import sleep\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def process_companies(company_chunk):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    wait = WebDriverWait(driver, 20)\n",
        "\n",
        "    base_url = \"https://kbopub.economie.fgov.be/kbopub/zoeknaamfonetischform.html?lang=en\"\n",
        "    result_chunk = []\n",
        "    successful_count = 0\n",
        "\n",
        "    company_types = [\n",
        "        \"VZW\", \"BVBA\", \"BV\", \"NV\", \"CV\", \"CVBA\", \"SPRL\", \"SCRL\", \"ASBL\",\n",
        "        \"Comm.V\", \"SComm\", \"VOF\", \"SNC\", \"GIE\", \"AIE\", \"SE\", \"Partnership\"\n",
        "    ]\n",
        "\n",
        "    def clean_company_name(company_name):\n",
        "        return re.sub(r'\\b(?:' + '|'.join(company_types) + r')\\b', '', company_name, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    sleep_time = 10\n",
        "    for company_name in company_chunk:\n",
        "        try:\n",
        "            clean_name = clean_company_name(company_name)\n",
        "            driver.get(base_url)\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "            sleep(sleep_time)\n",
        "            search_box = wait.until(EC.presence_of_element_located((By.ID, \"searchWord\")))\n",
        "            search_box.clear()\n",
        "            search_box.send_keys(clean_name)\n",
        "\n",
        "            checkbox = driver.find_element(By.ID, \"filterEnkelActieve\")\n",
        "            if checkbox.is_selected():\n",
        "                checkbox.click()\n",
        "\n",
        "            search_button = wait.until(EC.element_to_be_clickable((By.NAME, \"actionNPRP\")))\n",
        "            search_button.click()\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
        "                if \"no result found for this search term.\".lower() in page_text.lower():\n",
        "                    print(f\"No result for {company_name}\")\n",
        "                    result_chunk.append({\n",
        "                        'OriginalCompanyName': company_name,\n",
        "                        'CleanedCompanyName': clean_name,\n",
        "                        'Status': \"No result found for this search term\",\n",
        "                        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                    })\n",
        "                    continue\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            rows = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#onderneminglistfonetisch tbody tr')))\n",
        "            status = \"not found in KBO data table\"\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            for row in rows:\n",
        "                name_cell = row.find_element(By.CLASS_NAME, 'benaming').text.strip()\n",
        "                if name_cell.lower() == clean_name.lower():\n",
        "                    status_cell = row.find_elements(By.TAG_NAME, 'td')[1].text.strip()\n",
        "                    status = re.sub(r'\\s+', ' ', status_cell).strip()\n",
        "                    successful_count += 1\n",
        "                    break\n",
        "\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': status,\n",
        "                'Timestamp': timestamp\n",
        "            })\n",
        "\n",
        "        except (NoSuchElementException, TimeoutException, Exception) as e:\n",
        "            print(f\"Failed to process {company_name}\")\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': \"error\",\n",
        "                'Timestamp': \"N/A\"\n",
        "            })\n",
        "\n",
        "    driver.quit()\n",
        "    return result_chunk, successful_count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Load the Excel file\n",
        "    uploaded_file_path = 'BELGIUM_companies_short.xlsx'  # Replace with your actual file path\n",
        "    company_list = pd.read_excel(uploaded_file_path)['Name']\n",
        "\n",
        "    num_workers = 10\n",
        "\n",
        "    # Split the list of companies into chunks for multiprocessing\n",
        "    company_chunks = np.array_split(company_list, num_workers)\n",
        "    with Pool(num_workers) as pool:\n",
        "        results = pool.map(process_companies, company_chunks)\n",
        "\n",
        "    # Combine all results\n",
        "    all_results = [item[0] for item in results]\n",
        "    successful_count = sum(item[1] for item in results)\n",
        "    result_df = pd.DataFrame([item for sublist in all_results for item in sublist])\n",
        "\n",
        "    # Define the scoring dictionary\n",
        "    status_scores = {\n",
        "        \"ENT LP Active\": 1,\n",
        "        \"ENT LP Stopped\": 5,\n",
        "        \"error\": 2,\n",
        "        \"EU Active\": 1,\n",
        "        \"EU Stopped\": 5,\n",
        "        \"No result found for this search term\": 2,\n",
        "        \"not found in KBO data table\": 2\n",
        "    }\n",
        "\n",
        "    # Map the 'Status' column to scores based on the dictionary\n",
        "    result_df['Score'] = result_df['Status'].map(status_scores).fillna(0)\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    result_df.to_csv('Step_2_company_status_report_with_scores.csv', index=False)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total time taken: {end_time - start_time}\")\n",
        "    print(f\"Total successfully found statuses: {successful_count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbtYJWL3CL-E",
        "outputId": "b99b7c5d-00c9-4ee6-f043-582a39290a36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start time: 2024-12-03 15:40:30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No result for Zwick Roell Belux CV\n",
            "Failed to process Albatros Zweefvliegclub\n",
            "No result for Brugs Motoren Bedrijf nv\n",
            "No result for Van Laer-Mazet/Chris\n",
            "Failed to process Zzlite\n",
            "End time: 2024-12-03 15:41:39\n",
            "Total time taken: 0:01:08.513871\n",
            "Total successfully found statuses: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III STEP- web scrapping"
      ],
      "metadata": {
        "id": "a4KzImkXOlZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import re\n",
        "from googleapiclient.discovery import build\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'  # Replace with your Google API key\n",
        "google_cse_id = 'xxx'  # Replace with your Custom Search Engine ID\n",
        "\n",
        "# Load the CSV file to get company names\n",
        "df = pd.read_csv('Offshore Leaks-entities.csv', low_memory=False, encoding='utf-8')\n",
        "company_names = df['name'][0:20].tolist()  # Processing the first 20 companies\n",
        "\n",
        "# Keywords and associated scores\n",
        "keywords_score_30 = [\n",
        "    \"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\", \"criminal case\", \"arrested\"\n",
        "]\n",
        "keywords_score_5 = [\n",
        "    \"court\", \"accusation\", \"penalty\", \"investigation\", \"insolvency\", \"violation\", \"debt\", \"blackmail\"\n",
        "]\n",
        "keywords_score_0 = [\"stock\", \"stock price\"]\n",
        "\n",
        "score_no_words = 1\n",
        "\n",
        "def google_search(search_term, api_key, cse_id, start_index=1):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    try:\n",
        "        res = service.cse().list(q=search_term, cx=cse_id, start=start_index).execute()\n",
        "        return res.get('items', [])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to search for {search_term} with error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            if 'text/html' in response.headers.get('Content-Type', ''):\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                    script.extract()\n",
        "                for comment in soup.findAll(text=lambda text: isinstance(text, Comment)):\n",
        "                    comment.extract()\n",
        "                text = ' '.join(soup.stripped_strings)\n",
        "                return text\n",
        "            else:\n",
        "                return \"Non-text content skipped\"\n",
        "        else:\n",
        "            return \"\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with single space\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def calculate_score(text):\n",
        "    \"\"\"\n",
        "    Determine the score for a given text based on keyword matching.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    if any(keyword in text_lower for keyword in keywords_score_30):\n",
        "        return 30\n",
        "    elif any(keyword in text_lower for keyword in keywords_score_5):\n",
        "        return 5\n",
        "    elif any(keyword in text_lower for keyword in keywords_score_0):\n",
        "        return 0\n",
        "    elif text.strip() == \"\":\n",
        "        return score_no_words\n",
        "    else:\n",
        "        return score_no_words\n",
        "\n",
        "def process_company(company_name):\n",
        "    results = google_search(company_name, google_api_key, google_cse_id)\n",
        "    company_data = []\n",
        "    for result in results:\n",
        "        url = result['link']\n",
        "        extracted_text = extract_text_from_url(url)\n",
        "        if extracted_text != \"Non-text content skipped\":\n",
        "            extracted_text = clean_text(extracted_text)\n",
        "            score = calculate_score(extracted_text)\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': extracted_text,\n",
        "                'score': score\n",
        "            })\n",
        "        else:\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': \"Skipped due to non-text content\",\n",
        "                'score': score_no_words\n",
        "            })\n",
        "    return company_data\n",
        "\n",
        "# Use ThreadPoolExecutor to process companies in parallel\n",
        "data = []\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {executor.submit(process_company, name): name for name in company_names}\n",
        "    for future in as_completed(futures):\n",
        "        data.extend(future.result())\n",
        "\n",
        "# Convert list of dicts to DataFrame\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file with proper encoding and escaping\n",
        "output_file_path = 'Step_3_company_analysis_with_scores.csv'  # Replace with desired file path\n",
        "df_results.to_csv(output_file_path, index=False, escapechar='\\\\', encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "\n",
        "print(f\"Data saved to {output_file_path}.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PEj-5ZNlOiBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N_Vp8mfROiuj"
      }
    }
  ]
}