{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install pandas\n",
        "!pip install fuzzywuzzy\n",
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFZ4Whe53TKl",
        "outputId": "c42f1853-bacf-4d6d-989c-4997d45ef16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.12.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.27.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Step -Sanction list check"
      ],
      "metadata": {
        "id": "ZKnITnhz0rKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhxrU0ZFUugS",
        "outputId": "0ed0a12d-1c1a-4672-eea1-5358f1bfac14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanctioned names saved to 'sanction_list.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URLs for the CSV files\n",
        "url1 = 'https://www.treasury.gov/ofac/downloads/sdn.csv'\n",
        "url2 = 'https://www.treasury.gov/ofac/downloads/consolidated/cons_alt.csv'\n",
        "\n",
        "# Read data from the first CSV file\n",
        "df1 = pd.read_csv(url1, on_bad_lines='skip')\n",
        "sanction_list_url1 = df1.iloc[:, 1].dropna().unique()\n",
        "\n",
        "# Read data from the second CSV file\n",
        "df2 = pd.read_csv(url2, on_bad_lines='skip')\n",
        "sanction_list_url2 = df2.iloc[:, 3].dropna().unique()\n",
        "\n",
        "# Combine the names from both CSV files\n",
        "sanction_list = list(set(sanction_list_url1) | set(sanction_list_url2))\n",
        "\n",
        "# Create a DataFrame from the combined names\n",
        "sanction_list_df = pd.DataFrame({'Sanctioned Names': sanction_list})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "sanction_list_df.to_csv('sanction_list.csv', index=False)\n",
        "\n",
        "print(\"Sanctioned names saved to 'sanction_list.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Paths to the files\n",
        "uploaded_file_path = 'BELGIUM_companies_short.xlsx'\n",
        "sanction_list_file_path = 'sanction_list.csv'\n",
        "\n",
        "# Load the companies file\n",
        "companies_df = pd.read_excel(uploaded_file_path)\n",
        "\n",
        "# Load the sanction list\n",
        "sanction_list_df = pd.read_csv(sanction_list_file_path)\n",
        "\n",
        "# Normalize the sanction list for case-insensitive matching\n",
        "sanctioned_names = sanction_list_df['Sanctioned Names'].str.lower().tolist()\n",
        "\n",
        "# Function for approximate matching\n",
        "def approximate_match(name, sanctioned_names, threshold=85):\n",
        "    \"\"\" Check if a name approximately matches any sanctioned name.\n",
        "    : name: Name to match\n",
        "    :sanctioned_names: List of sanctioned names\n",
        "    :threshold: Minimum similarity score for a match\n",
        "    : 46 if a match is found, 0 otherwise\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    for sanctioned_name in sanctioned_names:\n",
        "        similarity = fuzz.ratio(name, sanctioned_name)\n",
        "        if similarity >= threshold:\n",
        "            return 46  # Match found\n",
        "    return 0  # No match\n",
        "\n",
        "# Evaluate if company names approximately match any name in the sanction list\n",
        "companies_df['Score_Step_1'] = companies_df['Name'].apply(\n",
        "    lambda name: approximate_match(name, sanctioned_names)\n",
        ")\n",
        "\n",
        "# Save the updated  new file\n",
        "output_file_path = 'Step_1_evaluated_companies.xlsx'\n",
        "companies_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Evaluation complete with approximate matching. Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udXl_T6-2r1x",
        "outputId": "0d18d9d0-e02d-47d8-d4ba-780e3bb6084d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Evaluation complete with approximate matching. Results saved to Step_1_evaluated_companies.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II STEP- Company status/ active check"
      ],
      "metadata": {
        "id": "5eVlnOsGAtwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from time import sleep\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def process_companies(company_chunk):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    wait = WebDriverWait(driver, 20)\n",
        "\n",
        "    base_url = \"https://kbopub.economie.fgov.be/kbopub/zoeknaamfonetischform.html?lang=en\"\n",
        "    result_chunk = []\n",
        "    successful_count = 0\n",
        "\n",
        "    company_types = [\n",
        "        \"VZW\", \"BVBA\", \"BV\", \"NV\", \"CV\", \"CVBA\", \"SPRL\", \"SCRL\", \"ASBL\",\n",
        "        \"Comm.V\", \"SComm\", \"VOF\", \"SNC\", \"GIE\", \"AIE\", \"SE\", \"Partnership\"\n",
        "    ]\n",
        "\n",
        "    def clean_company_name(company_name):\n",
        "        return re.sub(r'\\b(?:' + '|'.join(company_types) + r')\\b', '', company_name, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    sleep_time = 10\n",
        "    for company_name in company_chunk:\n",
        "        try:\n",
        "            clean_name = clean_company_name(company_name)\n",
        "            driver.get(base_url)\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "            sleep(sleep_time)\n",
        "            search_box = wait.until(EC.presence_of_element_located((By.ID, \"searchWord\")))\n",
        "            search_box.clear()\n",
        "            search_box.send_keys(clean_name)\n",
        "\n",
        "            checkbox = driver.find_element(By.ID, \"filterEnkelActieve\")\n",
        "            if checkbox.is_selected():\n",
        "                checkbox.click()\n",
        "\n",
        "            search_button = wait.until(EC.element_to_be_clickable((By.NAME, \"actionNPRP\")))\n",
        "            search_button.click()\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
        "                if \"no result found for this search term.\".lower() in page_text.lower():\n",
        "                    print(f\"No result for {company_name}\")\n",
        "                    result_chunk.append({\n",
        "                        'OriginalCompanyName': company_name,\n",
        "                        'CleanedCompanyName': clean_name,\n",
        "                        'Status': \"No result found for this search term\",\n",
        "                        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                    })\n",
        "                    continue\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            rows = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#onderneminglistfonetisch tbody tr')))\n",
        "            status = \"not found in KBO data table\"\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            for row in rows:\n",
        "                name_cell = row.find_element(By.CLASS_NAME, 'benaming').text.strip()\n",
        "                if name_cell.lower() == clean_name.lower():\n",
        "                    status_cell = row.find_elements(By.TAG_NAME, 'td')[1].text.strip()\n",
        "                    status = re.sub(r'\\s+', ' ', status_cell).strip()\n",
        "                    successful_count += 1\n",
        "                    break\n",
        "\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': status,\n",
        "                'Timestamp': timestamp\n",
        "            })\n",
        "\n",
        "        except (NoSuchElementException, TimeoutException, Exception) as e:\n",
        "            print(f\"Failed to process {company_name}\")\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': \"error\",\n",
        "                'Timestamp': \"N/A\"\n",
        "            })\n",
        "\n",
        "    driver.quit()\n",
        "    return result_chunk, successful_count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Load the Excel file\n",
        "    uploaded_file_path = 'BELGIUM_companies_short.xlsx'\n",
        "    company_list = pd.read_excel(uploaded_file_path)['Name']\n",
        "\n",
        "    num_workers = 10\n",
        "\n",
        "    # Split the list of companies into chunks for multiprocessing\n",
        "    company_chunks = np.array_split(company_list, num_workers)\n",
        "    with Pool(num_workers) as pool:\n",
        "        results = pool.map(process_companies, company_chunks)\n",
        "\n",
        "    # Combine all results\n",
        "    all_results = [item[0] for item in results]\n",
        "    successful_count = sum(item[1] for item in results)\n",
        "    result_df = pd.DataFrame([item for sublist in all_results for item in sublist])\n",
        "\n",
        "    # Define the scoring dictionary\n",
        "    status_scores = {\n",
        "        \"ENT LP Active\": 1,\n",
        "        \"ENT LP Stopped\": 5,\n",
        "        \"error\": 2,\n",
        "        \"EU Active\": 1,\n",
        "        \"EU Stopped\": 5,\n",
        "        \"No result found for this search term\": 2,\n",
        "        \"not found in KBO data table\": 2\n",
        "    }\n",
        "\n",
        "    # Map the 'Status' column to scores based on the dictionary\n",
        "    result_df['Score'] = result_df['Status'].map(status_scores).fillna(0)\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    result_df.to_csv('Step_2_company_status_report_with_scores.csv', index=False)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total time taken: {end_time - start_time}\")\n",
        "    print(f\"Total successfully found statuses: {successful_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbtYJWL3CL-E",
        "outputId": "e0c13b36-cf14-407b-872f-ef5eba34e760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start time: 2024-12-23 22:35:15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No result for Zwick Roell Belux CV\n",
            "No result for Van Laer-Mazet/Chris\n",
            "No result for Brugs Motoren Bedrijf nv\n",
            "No result for Zzlite\n",
            "End time: 2024-12-23 22:37:33\n",
            "Total time taken: 0:02:17.728790\n",
            "Total successfully found statuses: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "\n",
        "def process_companies(company_chunk):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    wait = WebDriverWait(driver, 45)\n",
        "\n",
        "    base_url = \"https://kbopub.economie.fgov.be/kbopub/zoeknaamfonetischform.html?lang=en\"\n",
        "    result_chunk = []\n",
        "    successful_count = 0\n",
        "\n",
        "    company_types = [\n",
        "    \"VZW\", \"BVBA\", \"BV\", \"NV\", \"CV\", \"CVBA\", \"SPRL\", \"SCRL\", \"ASBL\",\n",
        "    \"Comm.V\", \"SComm\", \"VOF\", \"SNC\", \"GIE\", \"AIE\", \"SE\", \"Partnership\"\n",
        "]\n",
        "    def clean_company_name(company_name):\n",
        "        return re.sub(r'\\b(?:' + '|'.join(company_types) + r')\\b', '', company_name, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    sleep_time = 15\n",
        "    for company_name in company_chunk:\n",
        "        try:\n",
        "            clean_name = clean_company_name(company_name)\n",
        "            driver.get(base_url)\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "            sleep(sleep_time)\n",
        "            search_box = wait.until(EC.presence_of_element_located((By.ID, \"searchWord\")))\n",
        "            search_box.clear()\n",
        "            search_box.send_keys(clean_name)\n",
        "\n",
        "            checkbox = driver.find_element(By.ID, \"filterEnkelActieve\")\n",
        "            if checkbox.is_selected():\n",
        "                checkbox.click()\n",
        "\n",
        "            search_button = wait.until(EC.element_to_be_clickable((By.NAME, \"actionNPRP\")))\n",
        "            search_button.click()\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
        "                if \"no result found for this search term.\".lower() in page_text.lower():\n",
        "                    print(f\"No result for {company_name}\")\n",
        "                    result_chunk.append({\n",
        "                        'OriginalCompanyName': company_name,\n",
        "                        'CleanedCompanyName': clean_name,\n",
        "                        'Status': \"No result found for this search term\",\n",
        "                        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                    })\n",
        "                    continue\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            rows = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#onderneminglistfonetisch tbody tr')))\n",
        "            status = \"not found in KBO data table\"\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            for row in rows:\n",
        "                name_cell = row.find_element(By.CLASS_NAME, 'benaming').text.strip()\n",
        "                if name_cell.lower() == clean_name.lower():\n",
        "                    status_cell = row.find_elements(By.TAG_NAME, 'td')[1].text.strip()\n",
        "                    status = re.sub(r'\\s+', ' ', status_cell).strip()\n",
        "                    successful_count += 1\n",
        "                    break\n",
        "\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': status,\n",
        "                'Timestamp': timestamp\n",
        "            })\n",
        "\n",
        "        except (NoSuchElementException, TimeoutException, Exception) as e:\n",
        "            print(f\"Failed to process {company_name}\")\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': \"error\",\n",
        "                'Timestamp': \"N/A\"\n",
        "            })\n",
        "\n",
        "    driver.quit()\n",
        "    return result_chunk, successful_count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    company_list = pd.read_csv('random_sample_2500_5.csv', encoding='latin-1', sep=',')['Name']\n",
        "    num_workers = 5\n",
        "\n",
        "    company_chunks = np.array_split(company_list, num_workers)\n",
        "    with Pool(num_workers) as pool:\n",
        "        results = pool.map(process_companies, company_chunks)\n",
        "    all_results = [item[0] for item in results]\n",
        "    successful_count = sum(item[1] for item in results)\n",
        "    result_df = pd.DataFrame([item for sublist in all_results for item in sublist])\n",
        "    result_df.to_csv('company_status_random_sample_2500_5.csv', index=False)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total time taken: {end_time - start_time}\")\n",
        "    print(f\"Total successfully found statuses: {successful_count}\")"
      ],
      "metadata": {
        "id": "oMnlQnvXbr3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III STEP- web scraping"
      ],
      "metadata": {
        "id": "a4KzImkXOlZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 STEP - web scraping using API"
      ],
      "metadata": {
        "id": "EsxKZdSkHjtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import re\n",
        "from googleapiclient.discovery import build\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'  # Replace with your Google API key\n",
        "google_cse_id = 'xxx'  # Replace with your Custom Search Engine ID\n",
        "\n",
        "# Load the CSV file to get company names\n",
        "#df = pd.read_csv('Offshore Leaks-entities.csv', low_memory=False, encoding='utf-8')\n",
        "df = pd.read_excel('BELGIUM_companies_short.xlsx', engine='openpyxl')\n",
        "#company_names = df['name'][0:20].tolist()  # Processing the first 20 companies\n",
        "\n",
        "# Keywords and associated scores\n",
        "keywords_score_30 = [\n",
        "    \"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\", \"criminal case\", \"arrested\"\n",
        "]\n",
        "keywords_score_5 = [\n",
        "    \"court\", \"accusation\", \"penalty\", \"investigation\", \"insolvency\", \"violation\", \"debt\", \"blackmail\"\n",
        "]\n",
        "keywords_score_minus_1 = [\"stock\"]  # Negative scoring for \"stock\"\n",
        "\n",
        "score_no_words = 0\n",
        "\n",
        "def google_search(search_term, api_key, cse_id, start_index=1):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    try:\n",
        "        res = service.cse().list(q=search_term, cx=cse_id, start=start_index).execute()\n",
        "        return res.get('items', [])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to search for {search_term} with error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            if 'text/html' in response.headers.get('Content-Type', ''):\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                    script.extract()\n",
        "                for comment in soup.findAll(text=lambda text: isinstance(text, Comment)):\n",
        "                    comment.extract()\n",
        "                text = ' '.join(soup.stripped_strings)\n",
        "                return text\n",
        "            else:\n",
        "                return \"Non-text content skipped\"\n",
        "        else:\n",
        "            return \"\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with single space\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def calculate_score(text):\n",
        "    \"\"\"\n",
        "    Determine the score for a given text based on keyword matching.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    if any(keyword in text_lower for keyword in keywords_score_30):\n",
        "        return 30\n",
        "    elif any(keyword in text_lower for keyword in keywords_score_5):\n",
        "        return 5\n",
        "    elif any(keyword in text_lower for keyword in keywords_score_minus_1):\n",
        "        return -1\n",
        "    elif text.strip() == \"\":\n",
        "        return score_no_words\n",
        "    else:\n",
        "        return score_no_words\n",
        "\n",
        "def process_company(company_name):\n",
        "    results = google_search(company_name, google_api_key, google_cse_id)\n",
        "    company_data = []\n",
        "    for result in results:\n",
        "        url = result['link']\n",
        "        extracted_text = extract_text_from_url(url)\n",
        "        if extracted_text != \"Non-text content skipped\":\n",
        "            extracted_text = clean_text(extracted_text)\n",
        "            score = calculate_score(extracted_text)\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': extracted_text,\n",
        "                'score': score\n",
        "            })\n",
        "        else:\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': \"Skipped due to non-text content\",\n",
        "                'score': score_no_words\n",
        "            })\n",
        "    return company_data\n",
        "\n",
        "# Use ThreadPoolExecutor to process companies in parallel\n",
        "data = []\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {executor.submit(process_company, name): name for name in company_names}\n",
        "    for future in as_completed(futures):\n",
        "        data.extend(future.result())\n",
        "\n",
        "# Convert list of dicts to DataFrame\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file with proper encoding and escaping\n",
        "output_file_path = 'Step_3.1_company_analysis_with_scores.csv'  # Replace with desired file path\n",
        "df_results.to_csv(output_file_path, index=False, escapechar='\\\\', encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "\n",
        "print(f\"Data saved to {output_file_path}.\")"
      ],
      "metadata": {
        "id": "PEj-5ZNlOiBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##V-2"
      ],
      "metadata": {
        "id": "LnUM0jg_0fKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import re\n",
        "from googleapiclient.discovery import build\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'  # Replace with your Google API key\n",
        "google_cse_id = 'xxx'   # Replace with your Custom Search Engine ID\n",
        "\n",
        "# Load the Excel file to get company names\n",
        "df = pd.read_excel('BELGIUM_companies_short.xlsx', engine='openpyxl')\n",
        "company_names = df['Name'][:500].tolist()  # Process 500 companies\n",
        "\n",
        "# Keywords and associated scores\n",
        "keywords_score_30 = [\n",
        "    \"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\", \"offshore\",\n",
        "    \"criminal case\", \"arrested\", \"fraud\", \"money laundering\",\n",
        "    \"embezzlement\", \"terrorism financing\", \"bribery\", \"tax evasion\",\n",
        "    \"illicit funds\", \"smuggling\", \"seized assets\", \"fines\",\n",
        "    \"indictment\", \"prosecuted\", \"wanted\", \"scam\", \"scandal\"\n",
        "]\n",
        "\n",
        "keywords_score_5 = [\n",
        "    \"court\", \"accusation\", \"penalty\", \"investigation\",\n",
        "    \"insolvency\", \"violation\", \"debt\", \"blackmail\", \"lawsuit\",\n",
        "    \"default\", \"litigation\", \"settlement\", \"audit\", \"suspicious\",\n",
        "    \"foreclosure\", \"dispute\", \"breach\", \"illegal transaction\",\n",
        "    \"arbitration\", \"compliance failure\", \"tax fraud\"\n",
        "]\n",
        "\n",
        "keywords_score_minus_1 = [\"stock\"]\n",
        "score_no_words = 0\n",
        "\n",
        "# Exclude domains\n",
        "exclude_domains = [\n",
        "    'dictionary.com', 'wiktionary.org', 'merriam-webster.com',\n",
        "    'facebook.com', 'twitter.com', 'vimeo.com', 'youtube.com',\n",
        "    'linkedin.com', 'reddit.com', 'quora.com', 'instagram.com',\n",
        "    'tiktok.com', 'pinterest.com', 'justia.com'\n",
        "]\n",
        "\n",
        "# Detect dictionary-like content from API results\n",
        "dictionary_keywords = [\n",
        "    \"definition\", \"meaning\", \"dictionary\", \"thesaurus\", \"pronunciation\"\n",
        "]\n",
        "\n",
        "\n",
        "# API-based Google Search\n",
        "def google_search(search_term, api_key, cse_id, start_index=1):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    try:\n",
        "        res = service.cse().list(q=search_term, cx=cse_id, start=start_index).execute()\n",
        "        return res.get('items', [])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to search for {search_term} with error: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Check if a URL belongs to an excluded domain\n",
        "def is_valid_url(url):\n",
        "    return not any(domain in url for domain in exclude_domains)\n",
        "\n",
        "\n",
        "# Filter out dictionary or irrelevant pages from API results\n",
        "def is_dictionary_page(api_result):\n",
        "    title = api_result.get('title', '').lower()\n",
        "    snippet = api_result.get('snippet', '').lower()\n",
        "\n",
        "    if any(keyword in title for keyword in dictionary_keywords):\n",
        "        return True\n",
        "    if any(keyword in snippet for keyword in dictionary_keywords):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Extract and clean text from URLs\n",
        "def extract_text_from_url(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "        if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove scripts and irrelevant tags\n",
        "            for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                script.extract()\n",
        "            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "                comment.extract()\n",
        "\n",
        "            return ' '.join(soup.stripped_strings)\n",
        "        else:\n",
        "            return \"Non-text content skipped\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "\n",
        "# Clean extracted text\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "\n",
        "# Calculate scores based on keyword proximity\n",
        "def calculate_score_with_reason(text, snippet, company_name):\n",
        "    text_lower = text.lower()\n",
        "    snippet_lower = snippet.lower()\n",
        "    matching_keywords = []\n",
        "    score = score_no_words\n",
        "\n",
        "    def keyword_in_same_sentence(keyword):\n",
        "        sentences = re.split(r'[.!?]', text_lower)\n",
        "        for sentence in sentences:\n",
        "            if company_name.lower() in sentence and keyword in sentence:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    for keyword in keywords_score_30:\n",
        "        if keyword_in_same_sentence(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 30\n",
        "\n",
        "    for keyword in keywords_score_5:\n",
        "        if keyword_in_same_sentence(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 5\n",
        "\n",
        "    for keyword in keywords_score_minus_1:\n",
        "        if keyword_in_same_sentence(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score -= 1\n",
        "\n",
        "    return score, matching_keywords or [\"No relevant keywords\"]\n",
        "\n",
        "\n",
        "# Process Each Company\n",
        "def process_company(company_name):\n",
        "    results = google_search(company_name, google_api_key, google_cse_id)\n",
        "    company_data = []\n",
        "\n",
        "    for result in results:\n",
        "        url = result['link']\n",
        "\n",
        "        # Skip dictionary or irrelevant results\n",
        "        if is_dictionary_page(result) or not is_valid_url(url):\n",
        "            continue\n",
        "\n",
        "        extracted_text = extract_text_from_url(url)\n",
        "        extracted_text = clean_text(extracted_text)\n",
        "\n",
        "        score, reasons = calculate_score_with_reason(extracted_text, result['snippet'], company_name)\n",
        "\n",
        "        company_data.append({\n",
        "            'company': company_name,\n",
        "            'url': url,\n",
        "            'extracted_text': extracted_text[:300],\n",
        "            'score': score,\n",
        "            'matched_keywords': ', '.join(reasons)\n",
        "        })\n",
        "\n",
        "    return company_data\n",
        "\n",
        "\n",
        "# Process companies in parallel\n",
        "data = []\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {executor.submit(process_company, name): name for name in company_names}\n",
        "    for future in as_completed(futures):\n",
        "        data.extend(future.result())\n",
        "\n",
        "df_results = pd.DataFrame(data)\n",
        "df_results.to_csv('Step_3.1_company_analysis_with_scores.csv', index=False, escapechar='\\\\', encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "\n",
        "print(\"Data saved successfully.\")"
      ],
      "metadata": {
        "id": "_GvcUzEksyv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 STEP - web scraping using BeautifulSoup"
      ],
      "metadata": {
        "id": "jY9e9kMIH0we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import re\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "import time\n",
        "from urllib.parse import quote\n",
        "import random\n",
        "\n",
        "# Load the CSV file to get company names\n",
        "df = pd.read_excel('BELGIUM_companies_short.xlsx', engine='openpyxl')\n",
        "#df = pd.read_csv('Offshore Leaks-entities.csv', low_memory=False, encoding='utf-8')\n",
        "company_names = df['Name'][0:20].tolist()  # Processing the first 20 companies\n",
        "\n",
        "# Keywords and associated scores\n",
        "keywords_score_30 = [\n",
        "    \"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\", \"offshore\",\n",
        "    \"criminal case\", \"arrested\", \"fraud\", \"money laundering\",\n",
        "    \"embezzlement\", \"terrorism financing\", \"bribery\", \"tax evasion\",\n",
        "    \"illicit funds\", \"smuggling\", \"seized assets\", \"fines\",\n",
        "    \"indictment\", \"prosecuted\", \"wanted\", \"scam\", \"scandal\"\n",
        "]\n",
        "\n",
        "keywords_score_5 = [\n",
        "    \"court\", \"accusation\", \"penalty\", \"investigation\",\n",
        "    \"insolvency\", \"violation\", \"debt\", \"blackmail\", \"lawsuit\",\n",
        "    \"default\", \"litigation\", \"settlement\", \"audit\", \"suspicious\",\n",
        "    \"foreclosure\", \"dispute\", \"breach\", \"illegal transaction\",\n",
        "    \"arbitration\", \"compliance failure\", \"tax fraud\"\n",
        "]\n",
        "\n",
        "keywords_score_minus_1 = [\"stock\"]  # Negative scoring\n",
        "\n",
        "score_no_words = 0\n",
        "\n",
        "# User-Agent Rotation\n",
        "user_agents = [\n",
        "\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)\",\n",
        "    \"Mozilla/5.0 (iPad; CPU OS 13_2 like Mac OS X)\"\n",
        "]\n",
        "\n",
        "def random_headers():\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(user_agents)\n",
        "    }\n",
        "\n",
        "# Domains to Exclude\n",
        "exclude_domains = [\n",
        "    'dictionary.com', 'wiktionary.org', 'merriam-webster.com',\n",
        "    'facebook.com', 'twitter.com', 'vimeo.com', 'youtube.com',\n",
        "    'linkedin.com', 'reddit.com', 'quora.com', 'instagram.com',\n",
        "    'tiktok.com', 'pinterest.com', 'justia.com'\n",
        "]\n",
        "\n",
        "def is_valid_url(url):\n",
        "    return not any(domain in url for domain in exclude_domains)\n",
        "\n",
        "# Bing Search Scraper\n",
        "def bing_search_scrape(company):\n",
        "    query = f'\"{company}\"'\n",
        "    url = f\"https://www.bing.com/search?q={quote(query)}\"\n",
        "    time.sleep(random.uniform(3, 7))\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=random_headers(), timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch results for {company}: {response.status_code}\")\n",
        "            return []\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        results = []\n",
        "\n",
        "        for g in soup.find_all('li', class_='b_algo'):\n",
        "            link_tag = g.find('a')\n",
        "            if not link_tag or 'href' not in link_tag.attrs:\n",
        "                continue\n",
        "\n",
        "            link = link_tag['href']\n",
        "            title = g.find('h2').text if g.find('h2') else \"\"\n",
        "            snippet = g.find('p').text if g.find('p') else \"\"\n",
        "\n",
        "            if not is_valid_url(link):\n",
        "                continue\n",
        "\n",
        "            if title or snippet:\n",
        "                results.append({\n",
        "                    \"title\": title,\n",
        "                    \"link\": link,\n",
        "                    \"snippet\": snippet\n",
        "                })\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping Bing for {company}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Keywords to detect dictionary-related content\n",
        "dictionary_keywords = [\n",
        "    \"definition\", \"meaning\", \"dictionary\", \"thesaurus\", \"pronunciation\"\n",
        "]\n",
        "\n",
        "# Function to check if the content is likely from a dictionary\n",
        "def is_dictionary_page(soup):\n",
        "    # Check title\n",
        "    if soup.title and any(word in soup.title.text.lower() for word in dictionary_keywords):\n",
        "        return True\n",
        "\n",
        "    # Check meta description\n",
        "    meta_description = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "    if meta_description and any(word in meta_description.get(\"content\", \"\").lower() for word in dictionary_keywords):\n",
        "        return True\n",
        "\n",
        "    # Check h1 or h2 headings\n",
        "    for tag in soup.find_all(['h1', 'h2']):\n",
        "        if any(word in tag.text.lower() for word in dictionary_keywords):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# Extract Relevant Text from URL (Updated)\n",
        "def extract_text_from_url(url, company_name):\n",
        "    try:\n",
        "        response = requests.get(url, headers=random_headers(), timeout=10)\n",
        "        if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Filter out dictionary pages\n",
        "            if is_dictionary_page(soup):\n",
        "                return \"Dictionary content skipped\"\n",
        "\n",
        "            # Clean unwanted sections\n",
        "            for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                script.extract()\n",
        "            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "                comment.extract()\n",
        "\n",
        "            # Extract relevant text\n",
        "            relevant_text = \"\"\n",
        "            for tag in soup.find_all(['h1', 'h2', 'h3', 'p']):\n",
        "                if company_name.lower() in tag.text.lower():\n",
        "                    relevant_text += tag.text + \" \"\n",
        "\n",
        "            if not relevant_text:\n",
        "                relevant_text = ' '.join(soup.stripped_strings)\n",
        "\n",
        "            return re.sub(r'\\s+', ' ', relevant_text[:2000])\n",
        "        else:\n",
        "            return \"Non-text content skipped\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "# Calculate Scores Based on Text\n",
        "def calculate_score_with_reason(text, snippet, company_name):\n",
        "    text_lower = text.lower()\n",
        "    snippet_lower = snippet.lower()\n",
        "    matching_keywords = []\n",
        "    score = score_no_words\n",
        "\n",
        "    # Direct sentence match for precision\n",
        "    def keyword_in_same_sentence(keyword):\n",
        "        sentences = re.split(r'[.!?]', text_lower)\n",
        "        for sentence in sentences:\n",
        "            if company_name.lower() in sentence and keyword in sentence:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # Proximity match for broader detection\n",
        "    def keyword_near_company(keyword):\n",
        "        match = re.search(rf\"\\b{keyword}\\b\", text_lower)\n",
        "        if match:\n",
        "            window = text_lower[max(0, match.start() - 500):match.end() + 500]\n",
        "            if company_name.lower() in window:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # High-risk keyword scoring\n",
        "    for keyword in keywords_score_30:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 30\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 30\n",
        "\n",
        "    # Medium-risk keyword scoring\n",
        "    for keyword in keywords_score_5:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 5\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 5\n",
        "\n",
        "    # Low-risk or neutral keywords\n",
        "    for keyword in keywords_score_minus_1:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score -= 1\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score -= 1\n",
        "\n",
        "\n",
        "    # Boost for financial distress mentions\n",
        "    if not matching_keywords:\n",
        "       for term in [\"insolvency\", \"bankruptcy\", \"liquidation\", \"dissolved\"]:\n",
        "        if term in text_lower:\n",
        "            score += 5\n",
        "            matching_keywords.append(term)\n",
        "\n",
        "    return score, matching_keywords or [\"No relevant keywords\"]\n",
        "\n",
        "# Process Each Company\n",
        "def process_company(company_name):\n",
        "    results = bing_search_scrape(company_name)\n",
        "    company_data = []\n",
        "\n",
        "    for result in results:\n",
        "        url = result['link']\n",
        "        snippet = result['snippet']\n",
        "        extracted_text = extract_text_from_url(url, company_name)\n",
        "\n",
        "        if extracted_text != \"Non-text content skipped\":\n",
        "            extracted_text = clean_text(extracted_text)\n",
        "            score, reasons = calculate_score_with_reason(extracted_text, snippet, company_name)\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': extracted_text[:300],\n",
        "                'score': score,\n",
        "                'matched_keywords': ', '.join(reasons)\n",
        "            })\n",
        "    return company_data\n",
        "\n",
        "# Multithreading for Efficiency\n",
        "data = []\n",
        "with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "    futures = {executor.submit(process_company, name): name for name in company_names}\n",
        "    for future in as_completed(futures):\n",
        "        data.extend(future.result())\n",
        "\n",
        "df_results = pd.DataFrame(data)\n",
        "df_results.to_csv('Step_3.2_company_analysis_with_scores.csv', index=False, encoding='utf-8', quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "\n",
        "print(\"Data saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZJWLE_WiNyt",
        "outputId": "479ee4fc-6ad6-4a68-af57-2aa586f0915d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# IV STEP- companies scoring\n",
        "\n"
      ],
      "metadata": {
        "id": "N_Vp8mfROiuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data files\n",
        "belgium_companies = pd.read_excel('BELGIUM_companies_short.xlsx')\n",
        "step_1 = pd.read_excel('Step_1_evaluated_companies.xlsx')\n",
        "step_2 = pd.read_csv('Step_2_company_status_report_with_scores.csv')\n",
        "step_3 = pd.read_csv('Step_3.2_company_analysis_with_scores.csv')\n",
        "\n",
        "# Standardize company name columns\n",
        "step_1.rename(columns={'Name': 'company_name'}, inplace=True)\n",
        "step_2.rename(columns={'OriginalCompanyName': 'company_name'}, inplace=True)\n",
        "step_3.rename(columns={'company': 'company_name'}, inplace=True)\n",
        "belgium_companies.rename(columns={'Name': 'company_name'}, inplace=True)\n",
        "\n",
        "# Combine all score files\n",
        "all_scores = pd.concat([step_1[['company_name', 'Score_Step_1']],\n",
        "                        step_2[['company_name', 'Score']],\n",
        "                        step_3[['company_name', 'score']].rename(columns={'score': 'Score'})],\n",
        "                       ignore_index=True)\n",
        "\n",
        "# Filter for companies in the Belgium list\n",
        "filtered_scores = all_scores[all_scores['company_name'].isin(belgium_companies['company_name'])]\n",
        "\n",
        "# Group by company and sum scores\n",
        "total_scores = filtered_scores.groupby('company_name')['Score'].sum().reset_index()\n",
        "\n",
        "# Apply risk level based on total score\n",
        "def assign_risk_level(score):\n",
        "    if score > 30:\n",
        "        return 'prohibited'\n",
        "    elif 7 <= score <= 30:\n",
        "        return 'high'\n",
        "    elif 1 <= score <= 6:\n",
        "        return 'medium'\n",
        "    elif score < 1:\n",
        "        return 'low'\n",
        "    else:\n",
        "        return 'no risk'\n",
        "\n",
        "# Assign risk levels\n",
        "total_scores['risk_level'] = total_scores['Score'].apply(assign_risk_level)\n",
        "\n",
        "# Save results to CSV\n",
        "total_scores.to_csv('Step_4_company_risk_scores.csv', index=False)\n",
        "print(\"Risk scoring completed and saved as 'Step_4_company_risk_scores.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uylK4_6alYZ9",
        "outputId": "ce93d9f8-38c7-49a6-cb2d-6a36fc54c48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk scoring completed and saved as 'Step_4_company_risk_scores.csv'\n"
          ]
        }
      ]
    }
  ]
}