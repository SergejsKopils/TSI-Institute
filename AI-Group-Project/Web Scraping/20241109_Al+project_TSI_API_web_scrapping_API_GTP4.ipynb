{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGoVnZM33AnWTRYKUdCMIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataKrj/AI-project-2024/blob/main/20241109_Al%2Bproject_TSI_API_web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORK GOOD** google 1 page"
      ],
      "metadata": {
        "id": "v3sAnXkS6mIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkf_7i-F6YwQ"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import openai\n",
        "import time\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'\n",
        "google_cse_id = 'xxx'\n",
        "openai.api_key = 'xxx'\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = [\n",
        "    \"court\", \"criminal case\", \"accusation\", \"crime\", \"corruption\", \"penalty\",\n",
        "    \"investigation\", \"insolvency\", \"debt\", \"violation\", \"arrested\", \"sanctions\",\n",
        "    \"litigation\", \"shell company\", \"blackmail\"\n",
        "]\n",
        "\n",
        "# Function to perform Google search\n",
        "def google_search(search_term, api_key, cse_id, **kwargs):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
        "    return res.get('items', [])\n",
        "\n",
        "# Function to extract text from a URL\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            text = ' '.join(p.text for p in soup.find_all('p'))\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "# Function to check if text contains any keywords\n",
        "def contains_keywords(text, keywords):\n",
        "    matched_keywords = [kw for kw in keywords if kw.lower() in text.lower()]\n",
        "    return \", \".join(matched_keywords) if matched_keywords else \"No match\"\n",
        "\n",
        "# Function to analyze text with GPT-4\n",
        "def analyze_text_with_gpt(text, company_name):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Determine if the following text is related to {company_name}.\"},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except openai.error.OpenAIError as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return \"Analysis failed.\"\n",
        "\n",
        "# Main process\n",
        "company_name = 'SwedBank'\n",
        "results = google_search(company_name, google_api_key, google_cse_id, num=10)\n",
        "\n",
        "# Create DataFrame to store results\n",
        "data = pd.DataFrame(columns=['company', 'url', 'extracted_text', 'gpt_analysis', 'related_keywords'])\n",
        "\n",
        "for result in results:\n",
        "    title = result['title']\n",
        "    link = result['link']\n",
        "\n",
        "    # Extract text from each URL\n",
        "    extracted_text = extract_text_from_url(link)\n",
        "\n",
        "    # Analyze extracted text with GPT-4 to determine relevance to the company\n",
        "    if extracted_text:\n",
        "        gpt_analysis = analyze_text_with_gpt(extracted_text, company_name)\n",
        "        related_keywords = contains_keywords(extracted_text, keywords)\n",
        "    else:\n",
        "        gpt_analysis = \"No text extracted.\"\n",
        "        related_keywords = \"No text extracted.\"\n",
        "\n",
        "    # Create a new row as a DataFrame and append it to the main DataFrame\n",
        "    new_row = pd.DataFrame({\n",
        "        'company': [company_name],\n",
        "        'url': [link],\n",
        "        'extracted_text': [extracted_text],\n",
        "        'gpt_analysis': [gpt_analysis],\n",
        "        'related_keywords': [related_keywords]  # Add matched keywords\n",
        "    })\n",
        "    data = pd.concat([data, new_row], ignore_index=True)  # Concatenate new row\n",
        "\n",
        "    # Delay to avoid rate limits\n",
        "    time.sleep(1)  # Adjust delay if necessary\n",
        "\n",
        "# Save the results to a CSV file\n",
        "data.to_csv('company_analysis_results.csv', index=False)\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORK GOOD** google 10 page"
      ],
      "metadata": {
        "id": "WQhnNuE56p6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import openai\n",
        "import time\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'\n",
        "google_cse_id = 'xxx'\n",
        "openai.api_key = 'xxx'\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = [\n",
        "    \"court\", \"criminal case\", \"accusation\", \"crime\", \"corruption\", \"penalty\",\n",
        "    \"investigation\", \"insolvency\", \"debt\", \"violation\", \"arrested\", \"sanctions\",\n",
        "    \"litigation\", \"shell company\", \"blackmail\"\n",
        "]\n",
        "\n",
        "# Function to perform Google search and get multiple pages\n",
        "def google_search(search_term, api_key, cse_id, num_pages=10):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    results = []\n",
        "\n",
        "    for page in range(num_pages):\n",
        "        start_index = page * 10 + 1  # 1, 11, 21, ... for each page\n",
        "        res = service.cse().list(q=search_term, cx=cse_id, start=start_index).execute()\n",
        "        if 'items' in res:\n",
        "            results.extend(res['items'])\n",
        "        time.sleep(1)  # Avoid rate limits by adding a delay\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to extract text from a URL\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            text = ' '.join(p.text for p in soup.find_all('p'))\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "# Function to check if text contains any keywords\n",
        "def contains_keywords(text, keywords):\n",
        "    matched_keywords = [kw for kw in keywords if kw.lower() in text.lower()]\n",
        "    return \", \".join(matched_keywords) if matched_keywords else \"No match\"\n",
        "\n",
        "# Function to analyze text with GPT-4\n",
        "def analyze_text_with_gpt(text, company_name):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Determine if the following text is related to {company_name}.\"},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except openai.error.OpenAIError as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return \"Analysis failed.\"\n",
        "\n",
        "# Main process\n",
        "company_name = 'SWEDBANK BALTICS, AS'\n",
        "results = google_search(company_name, google_api_key, google_cse_id, num_pages=10)\n",
        "\n",
        "# Create DataFrame to store results\n",
        "data = pd.DataFrame(columns=['company', 'url', 'extracted_text', 'gpt_analysis', 'related_keywords'])\n",
        "\n",
        "for result in results:\n",
        "    title = result['title']\n",
        "    link = result['link']\n",
        "\n",
        "    # Extract text from each URL\n",
        "    extracted_text = extract_text_from_url(link)\n",
        "\n",
        "    # Analyze extracted text with GPT-4 to determine relevance to the company\n",
        "    if extracted_text:\n",
        "        gpt_analysis = analyze_text_with_gpt(extracted_text, company_name)\n",
        "        related_keywords = contains_keywords(extracted_text, keywords)\n",
        "    else:\n",
        "        gpt_analysis = \"No text extracted.\"\n",
        "        related_keywords = \"No text extracted.\"\n",
        "\n",
        "    # Create a new row as a DataFrame and append it to the main DataFrame\n",
        "    new_row = pd.DataFrame({\n",
        "        'company': [company_name],\n",
        "        'url': [link],\n",
        "        'extracted_text': [extracted_text],\n",
        "        'gpt_analysis': [gpt_analysis],\n",
        "        'related_keywords': [related_keywords]  # Add matched keywords\n",
        "    })\n",
        "    data = pd.concat([data, new_row], ignore_index=True)  # Concatenate new row\n",
        "\n",
        "    # Delay to avoid rate limits\n",
        "    time.sleep(1)  # Adjust delay if necessary\n",
        "\n",
        "# Save the results to a CSV file\n",
        "data.to_csv('company_analysis_results.csv', index=False)\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "tlIH8jO76tXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20241118 NEW working fast script**"
      ],
      "metadata": {
        "id": "TC7a3Zk-3Xqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import string\n",
        "import re\n",
        "from googleapiclient.discovery import build\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "\n",
        "\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'\n",
        "google_cse_id = 'xxx'\n",
        "\n",
        "# Load the CSV file to get company names\n",
        "df = pd.read_csv('Offshore Leaks-entities.csv', low_memory=False, encoding='utf-8')\n",
        "company_names = df['name'][0:20].tolist()  # Slice from 1st to 10th company\n",
        "# company_names = df['name'].tolist()  # Slice from 1st to 10th company\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = [\n",
        "    \"court\", \"criminal case\", \"accusation\", \"crime\", \"corruption\", \"penalty\",\n",
        "    \"investigation\", \"insolvency\", \"debt\", \"violation\", \"arrested\", \"sanctions\",\n",
        "    \"litigation\", \"shell company\", \"blackmail\"\n",
        "]\n",
        "\n",
        "def google_search(search_term, api_key, cse_id, start_index=1):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    try:\n",
        "        res = service.cse().list(q=search_term, cx=cse_id, start=start_index).execute()\n",
        "        return res.get('items', [])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to search for {search_term} with error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            if 'text/html' in response.headers.get('Content-Type', ''):\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                    script.extract()\n",
        "                for comment in soup.findAll(text=lambda text: isinstance(text, Comment)):\n",
        "                    comment.extract()\n",
        "                text = ' '.join(soup.stripped_strings)\n",
        "                return text\n",
        "            else:\n",
        "                return \"Non-text content skipped\"\n",
        "        else:\n",
        "            return \"\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with single space\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def contains_keywords(text, keywords):\n",
        "    matched_keywords = [kw for kw in keywords if kw.lower() in text.lower()]\n",
        "    return \", \".join(matched_keywords) if matched_keywords else \"No match\"\n",
        "\n",
        "def classify_risk(related_keywords):\n",
        "    if any(keyword in related_keywords.lower() for keyword in [\"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\"]):\n",
        "        return \"High Risk\", 2\n",
        "    elif related_keywords != \"No match\" and related_keywords != \"\":\n",
        "        return \"Medium Risk\", 1\n",
        "    else:\n",
        "        return \"Low Risk\", 0\n",
        "\n",
        "def process_company(company_name):\n",
        "    results = google_search(company_name, google_api_key, google_cse_id)\n",
        "    company_data = []\n",
        "    for result in results:\n",
        "        url = result['link']\n",
        "        extracted_text = extract_text_from_url(url)\n",
        "        if extracted_text != \"Non-text content skipped\":\n",
        "            extracted_text = clean_text(extracted_text)\n",
        "            related_keywords = contains_keywords(extracted_text, keywords)\n",
        "            risk_level, risk_code = classify_risk(related_keywords)\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': extracted_text,\n",
        "                'related_keywords': related_keywords,\n",
        "                'risk_level': risk_level,\n",
        "                'risk_code': risk_code\n",
        "            })\n",
        "        else:\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': \"Skipped due to non-text content\",\n",
        "                'related_keywords': \"None\",\n",
        "                'risk_level': \"No Risk\",\n",
        "                'risk_code': 0\n",
        "            })\n",
        "    return company_data\n",
        "\n",
        "# Use ThreadPoolExecutor to process companies in parallel\n",
        "data = []\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {executor.submit(process_company, name): name for name in company_names}\n",
        "    for future in as_completed(futures):\n",
        "        data.extend(future.result())\n",
        "\n",
        "# Convert list of dicts to DataFrame\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file with proper encoding and escaping\n",
        "df_results.to_csv('1_0_20_company_analysis_results.csv', index=False, escapechar='\\\\', encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "print(\"Data saved to company_analysis_results.csv.\")\n",
        "\n",
        "# Additionally, save the DataFrame to a Parquet file\n",
        "df_results.to_parquet('1_0_20_company_analysis_results.parquet', engine='pyarrow', compression='snappy')\n",
        "print(\"Data saved to company_analysis_results.parquet.\")\n",
        "\n",
        "\n",
        "# Save the DataFrame to a JSON file\n",
        "df_results.to_json('1_0_20_company_analysis_results.json', orient='records', lines=True, force_ascii=False)\n",
        "print(\"Data saved to company_analysis_results.json.\")\n",
        "\n",
        "# Create an SQLAlchemy engine instance\n",
        "engine = create_engine('sqlite:///1_0_20_company_analysis_results_my_data.db')  # This will create a SQLite database file named 'my_data.db'\n",
        "\n",
        "# Assume df_results is your DataFrame\n",
        "df_results.to_sql('table_name', con=engine, index=False, if_exists='replace')  # Replace 'table_name' with your desired table name\n",
        "\n",
        "print(\"Data saved to the database.\")\n"
      ],
      "metadata": {
        "id": "etxrkRQd3aep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
