{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataKrj/AI-project-2024/blob/main/Step_1_sanction_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Sanction list check"
      ],
      "metadata": {
        "id": "ZKnITnhz0rKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhxrU0ZFUugS",
        "outputId": "f966d18c-f4b8-46b6-8f69-2f45b467e4d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanctioned names saved to 'sanction_list.csv'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# URLs for the CSV files\n",
        "url1 = 'https://www.treasury.gov/ofac/downloads/sdn.csv'\n",
        "url2 = 'https://www.treasury.gov/ofac/downloads/consolidated/cons_alt.csv'\n",
        "\n",
        "# Read data from the first CSV file\n",
        "df1 = pd.read_csv(url1, on_bad_lines='skip')\n",
        "sanction_list_url1 = df1.iloc[:, 1].dropna().unique()\n",
        "\n",
        "# Read data from the second CSV file\n",
        "df2 = pd.read_csv(url2, on_bad_lines='skip')\n",
        "sanction_list_url2 = df2.iloc[:, 3].dropna().unique()\n",
        "\n",
        "# Combine the names from both CSV files\n",
        "sanction_list = list(set(sanction_list_url1) | set(sanction_list_url2))\n",
        "\n",
        "# Create a DataFrame from the combined names\n",
        "sanction_list_df = pd.DataFrame({'Sanctioned Names': sanction_list})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "sanction_list_df.to_csv('sanction_list.csv', index=False)\n",
        "\n",
        "print(\"Sanctioned names saved to 'sanction_list.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Paths to the files\n",
        "uploaded_file_path = 'BELGIUM_companies_short.xlsx'\n",
        "sanction_list_file_path = 'sanction_list.csv'\n",
        "\n",
        "# Load the companies file\n",
        "companies_df = pd.read_excel(uploaded_file_path)\n",
        "\n",
        "# Load the sanction list\n",
        "sanction_list_df = pd.read_csv(sanction_list_file_path)\n",
        "\n",
        "# Normalize the sanction list for case-insensitive matching\n",
        "sanctioned_names = sanction_list_df['Sanctioned Names'].str.lower().tolist()\n",
        "\n",
        "# Function for approximate matching\n",
        "def approximate_match(name, sanctioned_names, threshold=85):\n",
        "    \"\"\" Check if a name approximately matches any sanctioned name.\n",
        "    : name: Name to match\n",
        "    :sanctioned_names: List of sanctioned names\n",
        "    :threshold: Minimum similarity score for a match\n",
        "    : 30 if a match is found, 0 otherwise\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    for sanctioned_name in sanctioned_names:\n",
        "        similarity = fuzz.ratio(name, sanctioned_name)\n",
        "        if similarity >= threshold:\n",
        "            return 30  # Match found\n",
        "    return 0  # No match\n",
        "\n",
        "# Evaluate if company names approximately match any name in the sanction list\n",
        "companies_df['Evaluation'] = companies_df['Name'].apply(\n",
        "    lambda name: approximate_match(name, sanctioned_names)\n",
        ")\n",
        "\n",
        "# Save the updated  new file\n",
        "output_file_path = 'Step_1_evaluated_companies.xlsx'\n",
        "companies_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Evaluation complete with approximate matching. Results saved to {output_file_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udXl_T6-2r1x",
        "outputId": "3fd8e9bc-e697-405b-9b0b-e2fe5962ac83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Evaluation complete with approximate matching. Results saved to Step_1_evaluated_companies.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2"
      ],
      "metadata": {
        "id": "5eVlnOsGAtwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from time import sleep\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def process_companies(company_chunk):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    wait = WebDriverWait(driver, 20)\n",
        "\n",
        "    base_url = \"https://kbopub.economie.fgov.be/kbopub/zoeknaamfonetischform.html?lang=en\"\n",
        "    result_chunk = []\n",
        "    successful_count = 0\n",
        "\n",
        "    company_types = [\n",
        "        \"VZW\", \"BVBA\", \"BV\", \"NV\", \"CV\", \"CVBA\", \"SPRL\", \"SCRL\", \"ASBL\",\n",
        "        \"Comm.V\", \"SComm\", \"VOF\", \"SNC\", \"GIE\", \"AIE\", \"SE\", \"Partnership\"\n",
        "    ]\n",
        "\n",
        "    def clean_company_name(company_name):\n",
        "        return re.sub(r'\\b(?:' + '|'.join(company_types) + r')\\b', '', company_name, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    sleep_time = 10\n",
        "    for company_name in company_chunk:\n",
        "        try:\n",
        "            clean_name = clean_company_name(company_name)\n",
        "            driver.get(base_url)\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "            sleep(sleep_time)\n",
        "            search_box = wait.until(EC.presence_of_element_located((By.ID, \"searchWord\")))\n",
        "            search_box.clear()\n",
        "            search_box.send_keys(clean_name)\n",
        "\n",
        "            checkbox = driver.find_element(By.ID, \"filterEnkelActieve\")\n",
        "            if checkbox.is_selected():\n",
        "                checkbox.click()\n",
        "\n",
        "            search_button = wait.until(EC.element_to_be_clickable((By.NAME, \"actionNPRP\")))\n",
        "            search_button.click()\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
        "                if \"no result found for this search term.\".lower() in page_text.lower():\n",
        "                    print(f\"No result for {company_name}\")\n",
        "                    result_chunk.append({\n",
        "                        'OriginalCompanyName': company_name,\n",
        "                        'CleanedCompanyName': clean_name,\n",
        "                        'Status': \"No result found for this search term\",\n",
        "                        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                    })\n",
        "                    continue\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            rows = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#onderneminglistfonetisch tbody tr')))\n",
        "            status = \"not found in KBO data table\"\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            for row in rows:\n",
        "                name_cell = row.find_element(By.CLASS_NAME, 'benaming').text.strip()\n",
        "                if name_cell.lower() == clean_name.lower():\n",
        "                    status_cell = row.find_elements(By.TAG_NAME, 'td')[1].text.strip()\n",
        "                    status = re.sub(r'\\s+', ' ', status_cell).strip()\n",
        "                    successful_count += 1\n",
        "                    break\n",
        "\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': status,\n",
        "                'Timestamp': timestamp\n",
        "            })\n",
        "\n",
        "        except (NoSuchElementException, TimeoutException, Exception) as e:\n",
        "            print(f\"Failed to process {company_name}\")\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': \"error\",\n",
        "                'Timestamp': \"N/A\"\n",
        "            })\n",
        "\n",
        "    driver.quit()\n",
        "    return result_chunk, successful_count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Load the Excel file\n",
        "    uploaded_file_path = 'BELGIUM_companies_short.xlsx'  # Replace with your actual file path\n",
        "    company_list = pd.read_excel(uploaded_file_path)['Name']\n",
        "\n",
        "    num_workers = 10\n",
        "\n",
        "    # Split the list of companies into chunks for multiprocessing\n",
        "    company_chunks = np.array_split(company_list, num_workers)\n",
        "    with Pool(num_workers) as pool:\n",
        "        results = pool.map(process_companies, company_chunks)\n",
        "\n",
        "    # Combine all results\n",
        "    all_results = [item[0] for item in results]\n",
        "    successful_count = sum(item[1] for item in results)\n",
        "    result_df = pd.DataFrame([item for sublist in all_results for item in sublist])\n",
        "\n",
        "    # Define the scoring dictionary\n",
        "    status_scores = {\n",
        "        \"ENT LP Active\": 1,\n",
        "        \"ENT LP Stopped\": 5,\n",
        "        \"error\": 2,\n",
        "        \"EU Active\": 1,\n",
        "        \"EU Stopped\": 5,\n",
        "        \"No result found for this search term\": 2,\n",
        "        \"not found in KBO data table\": 2\n",
        "    }\n",
        "\n",
        "    # Map the 'Status' column to scores based on the dictionary\n",
        "    result_df['Score'] = result_df['Status'].map(status_scores).fillna(0)\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    result_df.to_csv('Step_2_company_status_report_with_scores.csv', index=False)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total time taken: {end_time - start_time}\")\n",
        "    print(f\"Total successfully found statuses: {successful_count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbtYJWL3CL-E",
        "outputId": "a0a5c258-47a3-41c9-dc42-430815a990c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start time: 2024-12-03 15:25:29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No result for Zwick Roell Belux CV\n",
            "No result for Zzlite\n",
            "No result for Brugs Motoren Bedrijf nv\n",
            "No result for Van Laer-Mazet/Chris\n",
            "End time: 2024-12-03 15:26:20\n",
            "Total time taken: 0:00:51.179957\n",
            "Total successfully found statuses: 13\n"
          ]
        }
      ]
    }
  ]
}